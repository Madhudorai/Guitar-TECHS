{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMMyPZTLS9tM"
   },
   "source": [
    "# Installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eaQrtf2hHdK6",
    "outputId": "c7575fc9-d4a9-448d-cf92-a210455826cd"
   },
   "outputs": [],
   "source": [
    "!pip install pretty-midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mM4NxHsSsR1"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import librosa\n",
    "import pretty_midi\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtFDOv2_Tcyj"
   },
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_PLAYERS = ['P1', 'P2', 'P3']\n",
    "AVAILABLE_CONTENT = {\n",
    "    'P1': ['chords', 'scales', 'singlenotes', 'techniques'],\n",
    "    'P2': ['chords', 'scales', 'singlenotes', 'techniques'],\n",
    "    'P3': ['music']\n",
    "}\n",
    "VALID_MODALITIES = ['directinput', 'micamp', 'exo', 'ego']\n",
    "NUMSTRINGS =6 \n",
    "NUMFRETS = 25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn_dUntFgvD4"
   },
   "outputs": [],
   "source": [
    "class GuitarTECHSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch-compatible dataset for the Guitar-TECHS dataset\n",
    "    Args:\n",
    "        root_dir (str): Base directory where the dataset is stored (will be downloaded if missing).\n",
    "        sr (int): Sample rate to load audio at. Default is 48000 Hz.\n",
    "        players (list or 'all'): List of player IDs to include (e.g., ['P1', 'P2']) or 'all'.\n",
    "        content_types (list or 'all'): Content categories to include (e.g., ['chords', 'scales']) or 'all'.\n",
    "        modalities (list or 'all'): Audio/video types to load. Subset of ['directinput', 'micamp', 'exo', 'ego'] or 'all'.\n",
    "        slice_dur (float): Duration of each slice in seconds. Required for slicing.\n",
    "        slice_range (tuple): Alternative to slicing; fixed (start, end) time window in seconds.\n",
    "        slice_overlap (float): Overlap between slices in seconds.\n",
    "        label_bin_size (float): Duration of each time bin for MIDI labels in seconds (default: 0.1 #i.e 100ms).\n",
    "\n",
    "    Returns:\n",
    "        Each sample is a dictionary containing:\n",
    "            - 'player': Player ID (e.g., 'P1')\n",
    "            - 'content_type': Type of content (e.g., 'scales')\n",
    "            - 'sample': Unique sample name\n",
    "            - 'chord_type': (Optional) Chord category for chords\n",
    "            - 'data': Dictionary of audio/video modalities with sliced tensors\n",
    "            - 'label': Tensor of shape [6, 25, T] with note activations (string, fret, time_bin)\n",
    "            - 'midi_path': Path to original MIDI file\n",
    "            - 'slice_start' / 'slice_end': Start and end time (in seconds) of the current slice\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_dir='Guitar-TECHS',\n",
    "                 sr=48000,\n",
    "                 players=['all'],\n",
    "                 content_types='all',\n",
    "                 modalities='all',\n",
    "                 slice_dur=None,\n",
    "                 slice_range=None, \n",
    "                 slice_overlap=0.0, \n",
    "                 label_bin_size=0.1):\n",
    "\n",
    "        if slice_dur and slice_range:\n",
    "            raise ValueError(\"Cannot specify both slice_dur and slice_range.\")\n",
    "        if slice_overlap >= slice_dur:\n",
    "            raise ValueError(\"slice_overlap must be less than slice_dur.\")\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        if not os.path.exists(self.root_dir):\n",
    "           self._download_and_extract_dataset()\n",
    "        self.sr = sr\n",
    "        self.slice_dur = slice_dur\n",
    "        self.slice_range = slice_range\n",
    "        self.slice_overlap = slice_overlap\n",
    "        self.label_bin_size = label_bin_size\n",
    "        self.num_strings = NUMSTRINGS\n",
    "        self.num_frets = NUMFRETS\n",
    "\n",
    "        self.players = AVAILABLE_PLAYERS if players in ['all', ['all']] else players\n",
    "        assert all(p in AVAILABLE_PLAYERS for p in self.players), \\\n",
    "            f\"Players must be a subset of {AVAILABLE_PLAYERS}\"\n",
    "        self.modalities = VALID_MODALITIES if modalities in ['all', ['all']] else modalities\n",
    "        assert all(m in VALID_MODALITIES for m in self.modalities), \\\n",
    "            f\"Modalities must be a subset of {VALID_MODALITIES}\"\n",
    "\n",
    "        self.index = _build_index(self)\n",
    "\n",
    "        if self.slice_dur:\n",
    "          self.expanded_index = []\n",
    "          for i, sample_meta in enumerate(self.index):\n",
    "              base_dir = self._get_base_dir(sample_meta['player'], sample_meta['content_type'])\n",
    "              # use micamp for total length\n",
    "              audio_path = os.path.join(base_dir, 'audio', 'micamp', f\"micamp_{sample_meta['sample']}.wav\")\n",
    "\n",
    "              if not os.path.exists(audio_path):\n",
    "                  continue\n",
    "\n",
    "              duration = librosa.get_duration(path=audio_path)\n",
    "              total_samples = int(duration * self.sr)\n",
    "\n",
    "              # Load full audio\n",
    "              y, _ = librosa.load(audio_path, sr=self.sr)\n",
    "\n",
    "              slice_samples = int(self.slice_dur * self.sr)\n",
    "              overlap_samples = int(self.slice_overlap * self.sr)\n",
    "              hop_length = slice_samples - overlap_samples\n",
    "\n",
    "              # Pad the signal\n",
    "              pad_width = (slice_samples - len(y) % hop_length) % hop_length\n",
    "              y_padded = np.pad(y, (0, pad_width), mode='constant')\n",
    "\n",
    "              # Use librosa utils for slicing\n",
    "              frames = librosa.util.frame(y_padded, frame_length=slice_samples, hop_length=hop_length)\n",
    "\n",
    "              # For each frame, compute start and end time (in seconds)\n",
    "              for s in range(frames.shape[1]):\n",
    "                  start_sample = s * hop_length\n",
    "                  start_sec = start_sample / self.sr\n",
    "                  end_sec = start_sec + self.slice_dur\n",
    "                  self.expanded_index.append((i, start_sec, end_sec))\n",
    "\n",
    "\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"\n",
    "        # Build sample index by scanning directinput files.\n",
    "        :return: index on found audios\n",
    "        \"\"\"\n",
    "        local_index = []\n",
    "        for player in self.players:\n",
    "            valid_contents = AVAILABLE_CONTENT[player]\n",
    "            selected_contents = valid_contents if content_types in ['all', ['all']] else content_types\n",
    "            for content in selected_contents:\n",
    "                if content not in valid_contents:\n",
    "                    print(f\"Skipping content '{content}' for player '{player}' â€” not available in this player's dataset.\")\n",
    "                    continue\n",
    "                # Construct the base directory. Note: folder naming uses lower-case for content.\n",
    "                base_dir = self._get_base_dir(player, content)\n",
    "                di_dir = os.path.join(base_dir, 'audio', 'directinput')\n",
    "                if os.path.exists(di_dir):\n",
    "                    for fname in os.listdir(di_dir):\n",
    "                        if fname.startswith('directinput_') and fname.endswith('.wav'):\n",
    "                            # The sample identifier is based on the file name.\n",
    "                            sample_value = fname.replace('directinput_', '').replace('.wav', '')\n",
    "                            chord_type = None\n",
    "                            if content.lower() == 'chords':\n",
    "                                prefix = sample_value.split('_')[0]\n",
    "                                if prefix in ['Set1', 'Set2', 'Set3', 'Set4']:\n",
    "                                    chord_type = '3-note chord'\n",
    "                                elif prefix == 'Drop3':\n",
    "                                    chord_type = '4-note chord'\n",
    "                            local_index.append({\n",
    "                                'player': player,\n",
    "                                'content_type': content,\n",
    "                                'sample': sample_value,\n",
    "                                'chord_type': chord_type\n",
    "                            })\n",
    "        return local_index\n",
    "\n",
    "\n",
    "    def _download_and_extract_dataset(self):\n",
    "        \"\"\"\n",
    "        Downloads and extracts the Guitar-TECHS dataset if it's not already present.\n",
    "        Uses a progress bar to show download progress.\n",
    "        \"\"\"\n",
    "        print(f\"{self.root_dir} not found. Downloading dataset...\")\n",
    "\n",
    "        zip_path = \"dataset.zip\"\n",
    "        url = \"https://zenodo.org/api/records/14963133/files-archive\"\n",
    "\n",
    "        # Define known total size in bytes (3942.06 MB)\n",
    "        total_size = int(3942.06 * 1024 * 1024)\n",
    "        block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        with open(zip_path, 'wb') as f, tqdm(\n",
    "            desc=\"Downloading\",\n",
    "            total=total_size,\n",
    "            unit='iB',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                f.write(data)\n",
    "                bar.update(len(data))\n",
    "\n",
    "        print(\"Download complete. Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.root_dir)\n",
    "        os.remove(zip_path)\n",
    "\n",
    "        self._extract_nested_zip(self.root_dir)\n",
    "        print(\"Dataset downloaded and extracted successfully.\")\n",
    "\n",
    "\n",
    "    def _extract_nested_zip(self, root_dir):\n",
    "        \"\"\"\n",
    "        Recursively extracts all zip files found within the directory tree starting at root_dir.\n",
    "        After extraction, the original zip files are removed.\n",
    "        \"\"\"\n",
    "        for foldername, subfolders, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.zip'):\n",
    "                    zip_path = os.path.join(foldername, filename)\n",
    "                    extract_path = os.path.splitext(zip_path)[0]  # Folder name without .zip\n",
    "                    print(\"Extracting:\", zip_path, \"to\", extract_path)\n",
    "                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(extract_path)\n",
    "                    os.remove(zip_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expanded_index)\n",
    "\n",
    "    def _get_base_dir(self, player, content):\n",
    "        \"\"\"\n",
    "        Constructs the base directory for a given player and content type.\n",
    "        Expected naming: \"_\", and inside that folder may be a nested folder of the same name.\n",
    "        \"\"\"\n",
    "        dir_name = f\"{player}_{content.lower()}\"\n",
    "        candidate = os.path.join(self.root_dir, dir_name)\n",
    "        nested = os.path.join(candidate, dir_name)\n",
    "        return nested if os.path.exists(nested) else candidate\n",
    "\n",
    "    def _get_midi_path(self, item):\n",
    "        base_dir = self._get_base_dir(item['player'], item['content_type'])\n",
    "        return os.path.join(base_dir, 'midi', f\"midi_{item['sample']}.mid\")\n",
    "\n",
    "    def load_audio(self, path):\n",
    "        audio, _ = librosa.load(path, sr=self.sr)\n",
    "        return audio\n",
    "\n",
    "    def slice_audio(self, audio, start, end):\n",
    "        \"\"\"\n",
    "        Returns a slice of the audio corresponding to [start, end) seconds. When using slice_dur,\n",
    "        if the extracted segment is shorter than the desired slice length (i.e. (end - start) * sr),\n",
    "        it is padded with zeros at the end.\n",
    "        \"\"\"\n",
    "        start_sample = int(start * self.sr)\n",
    "        # Determine desired slice length in samples.\n",
    "        desired_length = int(self.slice_dur * self.sr) if self.slice_dur else int((end - start) * self.sr)\n",
    "        end_sample = start_sample + desired_length\n",
    "        segment = audio[start_sample: min(len(audio), end_sample)]\n",
    "        if len(segment) < desired_length:\n",
    "            segment = np.pad(segment, (0, desired_length - len(segment)), mode='constant')\n",
    "        return segment\n",
    "\n",
    "    def parse_midi(self, midi_obj, start=None, end=None):\n",
    "        \"\"\"\n",
    "        Converts PrettyMIDI object into a sparse tensor of shape [6, 26, T],\n",
    "        where T is the number of time bins based on label_bin_size.\n",
    "        Each entry is 1 if a note is active for that string/fret/time-bin.\n",
    "        Fret 25 (index) is used to explicitly denote silence.\n",
    "        \"\"\"\n",
    "        duration = end - start if (start is not None and end is not None) else midi_obj.get_end_time()\n",
    "        T = int(np.ceil(duration / self.label_bin_size))\n",
    "        label_tensor = torch.zeros((self.num_strings, self.num_frets + 1, T))  # +1 for silence\n",
    "\n",
    "        active_mask = torch.zeros((self.num_strings, T), dtype=torch.bool)\n",
    "\n",
    "        for string_index, instrument in enumerate(midi_obj.instruments):\n",
    "            for note in instrument.notes:\n",
    "                if start is not None and (note.end <= start or note.start >= end):\n",
    "                    continue\n",
    "\n",
    "                note_on = max(note.start, start) if start else note.start\n",
    "                note_off = min(note.end, end) if end else note.end\n",
    "                fret = self.pitch_to_fret(note.pitch)\n",
    "                if fret is None or fret >= self.num_frets:\n",
    "                    continue\n",
    "\n",
    "                onset_bin = int((note_on - start) / self.label_bin_size) if start else int(note_on / self.label_bin_size)\n",
    "                offset_bin = int((note_off - start) / self.label_bin_size) if start else int(note_off / self.label_bin_size)\n",
    "\n",
    "                onset_bin = max(0, min(T, onset_bin))\n",
    "                offset_bin = max(0, min(T, offset_bin))\n",
    "\n",
    "                label_tensor[string_index, fret, onset_bin:offset_bin] = 1\n",
    "                active_mask[string_index, onset_bin:offset_bin] = 1\n",
    "\n",
    "        # Mark silence explicitly with fret=25 where no other fret is active\n",
    "        for s in range(self.num_strings):\n",
    "            silence_bins = ~active_mask[s]\n",
    "            label_tensor[s, self.num_frets, silence_bins] = 1  # fret 25\n",
    "\n",
    "        return label_tensor\n",
    "\n",
    "    def pitch_to_fret(self, midi_note, tuning=[40, 45, 50, 55, 59, 64]):\n",
    "        for string_midi in tuning[::-1]:\n",
    "            fret = midi_note - string_midi\n",
    "            if 0 <= fret <= 24:\n",
    "                return fret\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx, start, end = self.expanded_index[idx]\n",
    "        item = self.index[real_idx]\n",
    "        base_dir = self._get_base_dir(item['player'], item['content_type'])\n",
    "\n",
    "        data = {}\n",
    "        # Load each modality.\n",
    "        for dtype in self.modalities:\n",
    "            if dtype in ['directinput', 'micamp']:\n",
    "                folder = os.path.join('audio', dtype)\n",
    "                ext = '.wav'\n",
    "            elif dtype in ['exo', 'ego']:\n",
    "                folder = os.path.join('video', dtype)\n",
    "                ext = '.mp3'\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            path = os.path.join(base_dir, folder, f\"{dtype}_{item['sample']}{ext}\")\n",
    "            if os.path.exists(path):\n",
    "                modality_data = self.load_audio(path)\n",
    "                # Slice (and pad if needed) the audio for the desired time window.\n",
    "                audio_array = self.slice_audio(modality_data, start, end) if start is not None else modality_data\n",
    "            if audio_array is not None:\n",
    "                data[dtype] = torch.from_numpy(audio_array).float()  # Convert to float tensor\n",
    "            else:\n",
    "                data[dtype] = None  # Or a zero tensor? \n",
    "\n",
    "\n",
    "        # Process MIDI labels for the corresponding time window.\n",
    "        midi_path = self._get_midi_path(item)\n",
    "        if os.path.exists(midi_path):\n",
    "            midi_obj = pretty_midi.PrettyMIDI(midi_path)\n",
    "            label_tensor = self.parse_midi(midi_obj, start, end)\n",
    "        else:\n",
    "            T = int((end - start) / self.label_bin_size)\n",
    "            label_tensor = torch.zeros((self.num_strings, self.num_frets, T))\n",
    "\n",
    "        # Return the sample dictionary including sample name and slice timestamps.\n",
    "        return {\n",
    "            'player': item['player'],\n",
    "            'content_type': item['content_type'],\n",
    "            'sample': item['sample'],\n",
    "            'chord_type': item.get('chord_type'),\n",
    "            'data': data,\n",
    "            'label': label_tensor,\n",
    "            'midi_path': midi_path,\n",
    "            'slice_start': start,\n",
    "            'slice_end': end\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJP-W8C-wWpR"
   },
   "source": [
    "#Initialising the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcCB___uIn_E",
    "outputId": "8e1e85c3-4145-448e-f66e-e88743f61e55"
   },
   "outputs": [],
   "source": [
    "dataset = GuitarTECHSDataset(\n",
    "    root_dir='Guitar-TECHS',\n",
    "    players=['P1'],\n",
    "    content_types=['all'],\n",
    "    modalities=['all'],\n",
    "    slice_dur=5, \n",
    "    slice_overlap=1,\n",
    "    label_bin_size =0.1 #100ms bin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFdHwQuj4RD2",
    "outputId": "a503b26b-5fee-43ca-d493-885f24b20695"
   },
   "outputs": [],
   "source": [
    "print(\"Number of samples/slices:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Looking at samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "id": "7BeJgH4V5YkW",
    "outputId": "24ce9494-e733-45c1-ce1f-99cdfe4e3e86"
   },
   "outputs": [],
   "source": [
    "# Get a sample from the dataset\n",
    "sample = dataset[1000]\n",
    "\n",
    "print(\"\\n *Sample Metadata*:\")\n",
    "pprint({k: v for k, v in sample.items() if k not in ['data', 'label']})\n",
    "\n",
    "print(\"\\n *Modalities in data* :\")\n",
    "for mod, tensor in sample['data'].items():\n",
    "    print(f\"  - {mod}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
    "\n",
    "print(\"\\n *Label tensor shape* :\")\n",
    "print(f\"  shape={sample['label'].shape}, dtype={sample['label'].dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor = sample['label']  # shape [6, 26, T] where T is number of time bins\n",
    "print(\"Label tensor shape:\", label_tensor.shape)\n",
    "\n",
    "nonzero_indices = torch.nonzero(label_tensor, as_tuple=False)\n",
    "label_df = pd.DataFrame(nonzero_indices.numpy(), columns=[\"string\", \"fret\", \"time_bin\"])\n",
    "\n",
    "# Adjust indexing\n",
    "label_df[\"string\"] += 1  # 0â€“5 â†’ 1â€“6\n",
    "label_df[\"fret\"] += 1    # 0â€“25 â†’ 1â€“26\n",
    "\n",
    "# Silence column: fret == 26 â†’ yes\n",
    "label_df[\"silence\"] = label_df[\"fret\"].apply(lambda f: \"yes\" if f == 26 else \"no\")\n",
    "\n",
    "# Display sorted by time_bin\n",
    "print(\"\\nTable of activations (indexed from 1-n):\")\n",
    "display(label_df.sort_values(by='time_bin').style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playback\n",
    "for modality in VALID_MODALITIES:\n",
    "    modality_data = sample['data'].get(modality)\n",
    "    if modality_data is not None:\n",
    "        print(f\"\\nðŸŽ§ Playing {modality} modality:\")\n",
    "        print(f\"  Shape: {modality_data.shape}\")\n",
    "        display(Audio(modality_data, rate=dataset.sr))\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ Modality '{modality}' is not available for this sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "\n",
    "print(\"\\n Batched Metadata:\")\n",
    "for key in ['player', 'content_type', 'sample', 'chord_type']:\n",
    "    print(f\"  {key}: {batch[key]}\")\n",
    "\n",
    "print(\"\\n Slice start/end times:\")\n",
    "print(f\"  slice_start: {batch['slice_start']}\")\n",
    "print(f\"  slice_end:   {batch['slice_end']}\")\n",
    "\n",
    "print(\"\\n Modalities in data:\")\n",
    "for mod, tensor in batch['data'].items():\n",
    "    print(f\"  - {mod}: shape={tensor.shape}, dtype={tensor.dtype}\") #shape is (batch size, audiosize) where audiosize = slice_dur Ã— sample_rate\n",
    "\n",
    "print(\"\\n Label tensor:\")\n",
    "print(f\"  shape={batch['label'].shape}, dtype={batch['label'].dtype}\")  #shape is (batch size, nstrings, nfrets+1, number of time bins), where timebins= slice_dur/label_bin_size\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
